{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CHAPTER 13 - Probabilistic Reasoning \n",
    "\n",
    "### George Tzanetakis, University of Victoria \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# WORKPLAN \n",
    "\n",
    "The section number is based on the 4th edition of the AIMA textbook and is the suggested\n",
    "reading for this week. Each list entry provides just the additional sections. For example the Expected reading include the sections listed under Basic as well as the sections listed under Expected. Some additional readings are suggested for Advanced. \n",
    "\n",
    "1. Basic: Sections **13.1**, **13.2 (not 13.2.1, 13.2.2, 13.2.3, 13.2.4)**, **13.3 (just exact inference)**, **13.4 (just direct sampling)**, and **Summary**\n",
    "2. Expected: Same as Basic + in 13.3 variable elimination) + in 1.34 (+ rejection sampling) \n",
    "3. Advanced: All the chapter including bibligraphical and historical notes \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inference using full joint distribution\n",
    "\n",
    "\n",
    "Let's consider another example where the full joint distribution $2 x 2 x 2$ is given. \n",
    "\n",
    "\n",
    "\n",
    "|---| toothache and catch    | toothache and not catch | not toothache and catch | not toothache and not catch | \n",
    "|---|-------   | ----------| ------| ----------|\n",
    "|cavity | 0.108 | 0.012 | 0.072 | 0.008 | \n",
    "| not cavity | 0.016 | 0.064 | 0.144 | 0.576 | \n",
    "\n",
    "\n",
    "Direct way to evalute the probability of any proposition: \n",
    "* Identify the possible worlds in which a proposition is true and add up their probabilities \n",
    "* $P(cavity \\lor toothache) = 0.108 + 0.012 + 0.072 + 0.008 + 0.016+ 0.064 = 0.28$ \n",
    "* **Marginal probability** of cavity: \n",
    "* $P(cavity) = 0.108 + 0.012 + 0.072 + 0.008 = 0.2$ \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The big picture \n",
    "\n",
    "We represent problems as sets variables with discrete and finite domains. The full joint probability distribution specifies probabilities for every possible assignment of all the variables to values from their corresponding domain. In a typical scenario we are given the values of some variables (called evidence) and are interested in the probability distribution of some variables (called query) given the evidence. The remaining variables are called the hidden variables. \n",
    "\n",
    "\n",
    "Without going into details, we can solve any inference problem by summation and products using the full joint probability distribution. \n",
    "\n",
    "The problem with this approach is that specifying the joint probability distribution becomes \n",
    "very difficult as the number of variables increases. For example if we have 10 binary random variables we would need to provide $2^{10}=1024$ probability values. Specifying probabilities \n",
    "\n",
    "We can take advantage of independence and conditional independence relationships among the random variables specifying our problem to greatly reduce the number of probabilities that need to be specified in order to define the full joint probability distribution. \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Bayesian Networks  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Specifying a Bayesian Network \n",
    "\n",
    "1. Each node corresponds to a random variable, which may be discrete or continuous \n",
    "2. Directed links connect pairs of nodes. If there is an arrow from node X to node Y, X is called the **parent** of Y. The graph has no directed cycles and hence is a directed acyclic graph, or DAG. \n",
    "3. Each node $X_{i}$ has associated probability information $\\theta(X_i|Parents(X_i))$ that quantifies the effect oif the parents on the node using a finite number of parameters. \n",
    "\n",
    "(Causes should be parents of effects - this is typically something that a domain expert can easily do) \n",
    "\n",
    "The joint distribution can be calculated from all the variables defined by the topology and the local probability information. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bayesian Network Example \n",
    "\n",
    "\n",
    "<img src=\"images/judea_pearl_bayes_net.png\" width=\"75%\"/>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Semantics of Bayesian Networks \n",
    "\n",
    "\n",
    "An entry in the joint probability distribution is: \n",
    "* $P(X_{1} = x_{1} \\wedge ... \\wedge X_{n} = x_{n})$ or $P(x_1,x_2,..,x_{n})$\n",
    "\n",
    "The Bayesian Network can be used to calculate each entry \"on demand\" as follows: \n",
    "\n",
    "* $P(x_1,x_2,..,x_{n}) = \\prod_{i=1}^{n} \\theta(x_{i} | parents(X_{i}))$\n",
    "\n",
    "Let's illustrate this with an example based on the network above: \n",
    "\n",
    "* $P(j,m,a,\\lnot b, \\lnot e) = P(j|a) P(m|a) P(a| \\lnot b \\lnot e) P(\\lnot b) P(\\lnot e) = 0.90 \\times 0.70 \\times 0.001 \\times 0.999 \\times 0.998 = 0.000628 $\n",
    "\n",
    "\n",
    "As the number of variables increases specifying the full joint probability distribution requires many more numbers than specifying a Bayesian Network. If we have $n$ boolen variables and each variables has $k$ parents \n",
    "the complete network can be specified by $2^{k} \\cdot n$ numbers. \n",
    "\n",
    "For example suppose that you have 30 variables and each one has 5 parents. The Baysian Network requires $32 * 30=960$ but the full distribution requires over a billion. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Constructing Bayesian Network \n",
    "\n",
    "\n",
    "Ideally parents of a node contain all the nodes that directly influence that node. \n",
    "It is possible for the same joint probability distribution to be represented by networks \n",
    "created by adding nodes in different orders. These networks can be clunky and hard to understand. \n",
    "\n",
    "\n",
    "Intuitively:\n",
    "* Start from root causes and expand effects –(follow causality) \n",
    "* For details: Read textbook\n",
    "\n",
    "<img src=\"images/different_orders_bayes_net.png\" width=\"75%\"/>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exact Inference by Enumeration\n",
    "\n",
    "\n",
    "The basic task of probabilistic inference system is to compute the posterior probability distribution for a set of **query variables**, given some observed **event** which is an assignment of values to a set of **evidence variables**. For presentation we will look into single query variables. The remaining variables are called **hidden**. \n",
    "\n",
    "**Note it is use to express multiple query variables in terms of single variable queries ** \n",
    "* Simple queries $P(NoGas | Gauge = empty, Lights = on, Starts =false)$\n",
    "* Conjuctive queries $P(Xi , Xj | E = e) = P(Xi| E = e) P(Xj| Xi,, E =e)$ \n",
    "\n",
    "For example in the burglary network: \n",
    "\n",
    "* $P(Burglary | JohnCalls = true, MaryCalls=true) = 0.716\n",
    "\n",
    "### Inference by enumeration \n",
    "\n",
    "$P(X,e) = \\alpha P(X,e) = \\alpha \\sum_{y} P(X,e,y)$\n",
    "\n",
    "So basically any query can be answered using a Bayes net by computing sum of products of conditional probabilities from the network. \n",
    "\n",
    "\n",
    "For example: \n",
    "\n",
    "* $P(b | j,m) = \\alpha \\sum_{e} \\sum_{a} P(b) P(e) P(a|b,e) P(j|a) P(m|a) = \\alpha P(b) \\sum_e P(e) \\sum_a P(a|b,e) P(j|a) P(m|a)$\n",
    "\n",
    "Note the use of $\\alpha$ for normalization: \n",
    "\n",
    "* $ P(B|j,m) = \\alpha <0.00059224, 0.0014919> \\approx <0.248, 0.716> $\n",
    "\n",
    "The change of burglary if both mary and john call. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"images/bayes_inference_enumeration.png\" width=\"75%\"/>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A bigger example of a Bayes net \n",
    "\n",
    "<img src=\"images/car_diagnostics_bayes.png\" width=\"75%\"/>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Additional topics \n",
    "\n",
    "\n",
    "* Markov blankets, ancestral graph, moral graph \n",
    "* Efficient representation of conditional distributions \n",
    "* Bayesian networks with continuous variables \n",
    "* Car insurance case study \n",
    "* Variable elimination algorithm \n",
    "* Complexity of exact inference \n",
    "* Clustering algorithms \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Approximate Inference "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The basic idea is that instead of calculating the exact probabilities of events we are interested in we can simulate stochastically the generation of samples using the network and simply count. Approximate inference can be much more efficient than exact inference for large networks. \n",
    "\n",
    "\n",
    "* Draw N samples from a sampling distribution \n",
    "*Compute an approximate posterior P\n",
    "* Show this converges to true prob. \n",
    "\n",
    "Examples of approximate inference: \n",
    "\n",
    "* Sampling from an empty network \n",
    "* Rejection sampling: reject samples disagreeing with evidence \n",
    "* Likelihood weighting: use evidence to weight samples \n",
    "* Markov Chain Monte Carlo (MCMC): sample from a stochastic process whose stationary distribution approximates the distribution "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sampling from a network with no evidence "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
